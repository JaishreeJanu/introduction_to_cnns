{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - 4 Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:27:42.756717Z",
     "start_time": "2021-06-27T08:27:41.142268Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show\n",
    "from bokeh.models import LinearAxis, Range1d\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "torch.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:27:46.211510Z",
     "start_time": "2021-06-27T08:27:46.203761Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 6\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "DATA_PATH = '/home/jaishree/Documents/DA Sem 1/DDA Lab/CNN_image_classification/cifar-10'\n",
    "MODEL_PATH = '/home/jaishree/Documents/DA Sem 1/DDA Lab/CNN_image_classification/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:27:55.787755Z",
     "start_time": "2021-06-27T08:27:53.861544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CIFAR10(root = DATA_PATH, download=True, transform=ToTensor())\n",
    "test_dataset = CIFAR10(root = DATA_PATH, train=False, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:27:58.543856Z",
     "start_time": "2021-06-27T08:27:58.497349Z"
    }
   },
   "outputs": [],
   "source": [
    "#Data Augmentation: the process of artificially ’increasing’ our dataset by adding translation, scaling \n",
    "#and flipping to the images to fabricate examples for training.\n",
    "\n",
    "train_len = 25*1000\n",
    "part_train = torch.utils.data.random_split(train_dataset, [train_len, len(train_dataset)-train_len])[0]\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=part_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:28:07.296493Z",
     "start_time": "2021-06-27T08:28:07.267184Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(7*7*128, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)       \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc_layer(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:28:13.113340Z",
     "start_time": "2021-06-27T08:28:12.613343Z"
    }
   },
   "outputs": [],
   "source": [
    "#model = ConvNet()\n",
    "\n",
    "# Loss and opitimizer\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_values = [0.001, 0.01, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T09:00:21.263196Z",
     "start_time": "2021-06-26T08:37:14.285181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Step [250/250], training Loss: 0.0351, training Accuracy: 17.59%\n",
      "Epoch [2/6], Step [250/250], training Loss: 0.0298, training Accuracy: 32.54%\n",
      "Epoch [3/6], Step [250/250], training Loss: 0.0243, training Accuracy: 42.88%\n",
      "Epoch [4/6], Step [250/250], training Loss: 0.0225, training Accuracy: 48.75%\n",
      "Epoch [5/6], Step [250/250], training Loss: 0.0211, training Accuracy: 52.76%\n",
      "Epoch [6/6], Step [250/250], training Loss: 0.0198, training Accuracy: 56.17%\n",
      "LR: 54.71, Test Accuracy of the model on all test images: 0.001 %\n",
      "Epoch [1/6], Step [250/250], training Loss: 0.0420, training Accuracy: 9.73%\n",
      "Epoch [2/6], Step [250/250], training Loss: 0.0330, training Accuracy: 10.13%\n",
      "Epoch [3/6], Step [250/250], training Loss: 0.0327, training Accuracy: 10.24%\n",
      "Epoch [4/6], Step [250/250], training Loss: 0.0327, training Accuracy: 10.28%\n",
      "Epoch [5/6], Step [250/250], training Loss: 0.0327, training Accuracy: 10.11%\n",
      "Epoch [6/6], Step [250/250], training Loss: 0.0327, training Accuracy: 10.11%\n",
      "LR: 10.0, Test Accuracy of the model on all test images: 0.01 %\n",
      "Epoch [1/6], Step [250/250], training Loss: 4213.2035, training Accuracy: 9.94%\n",
      "Epoch [2/6], Step [250/250], training Loss: 0.0286, training Accuracy: 10.07%\n",
      "Epoch [3/6], Step [250/250], training Loss: 0.0286, training Accuracy: 9.90%\n",
      "Epoch [4/6], Step [250/250], training Loss: 0.0286, training Accuracy: 9.84%\n",
      "Epoch [5/6], Step [250/250], training Loss: 0.0286, training Accuracy: 9.95%\n",
      "Epoch [6/6], Step [250/250], training Loss: 0.0286, training Accuracy: 9.90%\n",
      "LR: 10.0, Test Accuracy of the model on all test images: 0.1 %\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# Adam Optimizer\n",
    "\n",
    "for l_rate in param_values:\n",
    "    \n",
    "    model = ConvNet()\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(part_train,batch_size = batch_size, shuffle = True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=l_rate)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    #train_loss = []\n",
    "    #train_acc = []\n",
    "\n",
    "    tb = SummaryWriter()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "            # Run the forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            #train_loss.append(loss.item())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track the accuracy\n",
    "            total = labels.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            #train_acc.append(correct / total)\n",
    "\n",
    "        tb.add_scalar(\"Adam1 Train Loss\", total_loss/len(part_train), epoch)\n",
    "        tb.add_scalar(\"Adam1 Train Accuracy\", total_correct/len(part_train), epoch)\n",
    "\n",
    "        print('Epoch [{}/{}], Step [{}/{}], training Loss: {:.4f}, training Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, total_loss/len(part_train),\n",
    "                          (total_correct / len(part_train)) * 100))\n",
    "        \n",
    "        tb.add_hparams(\n",
    "            {\"lr\": l_rate},\n",
    "            {\n",
    "                \"accuracy\": total_correct/ len(part_train),\n",
    "                \"loss\": total_loss/len(part_train),\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "    tb.close()\n",
    "    \n",
    "    ## testing the model\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    int_correct = 0\n",
    "    int_total = 0\n",
    "    int_total_loss = 0\n",
    "\n",
    "    tb = SummaryWriter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (int_images, int_labels) in enumerate(test_loader):\n",
    "            int_outputs = model(int_images)\n",
    "            int_loss = criterion(int_outputs, int_labels)\n",
    "            test_int_loss.append(int_loss.item())\n",
    "            int_total_loss += int_loss.item()\n",
    "\n",
    "            _, predicted = torch.max(int_outputs.data, 1)\n",
    "            int_total += int_labels.size(0)\n",
    "            int_correct += (predicted == int_labels).sum().item()\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            test_int_acc.append(correct / int_labels.size(0))\n",
    "\n",
    "        tb.add_scalar(\"Adam1 Test Loss\", int_total_loss/len(test_dataset), l_rate)\n",
    "        tb.add_scalar(\"Adam1 Test Accuracy\", int_correct / int_total, l_rate)\n",
    "\n",
    "        print('LR: {}, Test Accuracy of the model on all test images: {} %'.format(l_rate, (int_correct / int_total) * 100))\n",
    "\n",
    "\n",
    "    tb.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Step [250/250], training Loss: 0.0474, training Accuracy: 9.67%\n",
      "Epoch [2/6], Step [250/250], training Loss: 0.0371, training Accuracy: 13.75%\n",
      "Epoch [3/6], Step [250/250], training Loss: 0.0359, training Accuracy: 15.69%\n",
      "Epoch [4/6], Step [250/250], training Loss: 0.0357, training Accuracy: 16.52%\n",
      "Epoch [5/6], Step [250/250], training Loss: 0.0353, training Accuracy: 19.19%\n",
      "Epoch [6/6], Step [250/250], training Loss: 0.0344, training Accuracy: 23.04%\n",
      "LR: 0.001, Test Accuracy of the model on all test images: 23.580000000000002 %\n",
      "Epoch [1/6], Step [250/250], training Loss: 0.0389, training Accuracy: 10.17%\n",
      "Epoch [2/6], Step [250/250], training Loss: 0.0346, training Accuracy: 17.67%\n",
      "Epoch [3/6], Step [250/250], training Loss: 0.0276, training Accuracy: 26.36%\n",
      "Epoch [4/6], Step [250/250], training Loss: 0.0163, training Accuracy: 39.28%\n",
      "Epoch [5/6], Step [250/250], training Loss: 0.0147, training Accuracy: 45.68%\n",
      "Epoch [6/6], Step [250/250], training Loss: 0.0133, training Accuracy: 51.94%\n",
      "LR: 0.01, Test Accuracy of the model on all test images: 52.81 %\n",
      "Epoch [1/6], Step [250/250], training Loss: 0.0319, training Accuracy: 10.04%\n",
      "Epoch [2/6], Step [250/250], training Loss: 0.0286, training Accuracy: 9.90%\n",
      "Epoch [3/6], Step [250/250], training Loss: 0.0286, training Accuracy: 10.08%\n",
      "Epoch [4/6], Step [250/250], training Loss: 0.0286, training Accuracy: 10.21%\n",
      "Epoch [5/6], Step [250/250], training Loss: 0.0286, training Accuracy: 9.90%\n",
      "Epoch [6/6], Step [250/250], training Loss: 0.0286, training Accuracy: 10.06%\n",
      "LR: 0.1, Test Accuracy of the model on all test images: 10.0 %\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "# SGD Optimizer\n",
    "\n",
    "for l_rate in param_values:\n",
    "    \n",
    "    model = ConvNet()\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(part_train,batch_size = batch_size, shuffle = True)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=l_rate, momentum=0.9)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    #train_loss = []\n",
    "    #train_acc = []\n",
    "\n",
    "    tb = SummaryWriter()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "            # Run the forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            #train_loss.append(loss.item())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track the accuracy\n",
    "            total = labels.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            #train_acc.append(correct / total)\n",
    "\n",
    "        tb.add_scalar(\"SGD Train Loss\", total_loss/len(part_train), epoch)\n",
    "        tb.add_scalar(\"SGD Train Accuracy\", total_correct/len(part_train), epoch)\n",
    "\n",
    "        print('Epoch [{}/{}], Step [{}/{}], training Loss: {:.4f}, training Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, total_loss/len(part_train),\n",
    "                          (total_correct / len(part_train)) * 100))\n",
    "        \n",
    "        tb.add_hparams(\n",
    "            {\"lr\": l_rate},\n",
    "            {\n",
    "                \"accuracy\": total_correct/ len(part_train),\n",
    "                \"loss\": total_loss/len(part_train),\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "    tb.close()\n",
    "    \n",
    "    \n",
    "    ## test step\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    int_correct = 0\n",
    "    int_total = 0\n",
    "    int_total_loss = 0\n",
    "\n",
    "    tb = SummaryWriter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (int_images, int_labels) in enumerate(test_loader):\n",
    "            int_outputs = model(int_images)\n",
    "            int_loss = criterion(int_outputs, int_labels)\n",
    "            test_int_loss.append(int_loss.item())\n",
    "            int_total_loss += int_loss.item()\n",
    "\n",
    "            _, predicted = torch.max(int_outputs.data, 1)\n",
    "            int_total += int_labels.size(0)\n",
    "            int_correct += (predicted == int_labels).sum().item()\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            test_int_acc.append(correct / int_labels.size(0))\n",
    "\n",
    "        tb.add_scalar(\"SGD Test Loss\", int_total_loss/len(test_dataset), l_rate)\n",
    "        tb.add_scalar(\"SGD Test Accuracy\", int_correct / int_total, l_rate)\n",
    "\n",
    "        print('LR: {}, Test Accuracy of the model on all test images: {} %'.format(l_rate, (int_correct / int_total) * 100))\n",
    "\n",
    "\n",
    "    tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
